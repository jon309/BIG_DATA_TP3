# -*- coding: utf-8 -*-
"""bigdata-TP4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KQIW0h-B2sr46gexuijgiY2Iw910U6L8
"""

input_csv = "/Real_Estate_Sales_2001-2022_GL.csv"
output_dir = "/output"

import pandas as pd
import os

file_path = os.getenv("input_csv", "Real_Estate_Sales_2001-2022_GL.csv") #/content/drive/MyDrive/Colab Notebooks/Real_Estate_Sales_2001-2022_GL.csv
df = pd.read_csv(file_path, low_memory=False)
df.head()
df.info()

import pandas as pd
import numpy as np

file_path = "Real_Estate_Sales_2001-2022_GL.csv" #/content/drive/MyDrive/Colab Notebooks/Real_Estate_Sales_2001-2022_GL.csv

dtype_text = {
    "Property Type": "string",
    "Residential Type": "string",
    "Non Use Code": "string",
    "Assessor Remarks": "string",
    "OPM remarks": "string",
    "Town": "string",
    "Address": "string",
    "Location": "string"
}

df = pd.read_csv(file_path, dtype=dtype_text, low_memory=False)

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ§Ø±ÙŠØ®
df["Date Recorded"] = pd.to_datetime(df["Date Recorded"], errors="coerce")

for col in ["Assessed Value", "Sale Amount", "Sales Ratio"]:
    df[col] = pd.to_numeric(df[col], errors="coerce")

for col in ["Town", "Property Type", "Residential Type", "Non Use Code"]:
    df[col] = df[col].astype("category")

df.info(memory_usage="deep")

"""Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""

print("Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ÙÙŠ ÙƒÙ„ Ø¹Ù…ÙˆØ¯:")
missing = df.isna().sum().sort_values(ascending=False)
print(missing)

# Ù†Ø­ØªÙØ¸ Ø¨Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªØ§Ø±ÙŠØ® Ùˆ Ù…Ø¨Ù„Øº Ø¨ÙŠØ¹
df = df.dropna(subset=["Date Recorded", "Sale Amount"])

# Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª ØµÙØ± Ø£Ùˆ Ø£Ù‚Ù„
df = df[df["Sale Amount"] > 0]

# Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª Ù‚ÙŠÙ… ØªÙ‚ÙŠÙŠÙ… (Assessed Value) Ø³Ø§Ù„Ø¨Ø© Ø£Ùˆ ØºÙŠØ± Ù…Ù†Ø·Ù‚ÙŠØ© Ù†Ø­Ø°ÙÙ‡Ø§
df = df[df["Assessed Value"] >= 0]

print(f"\n Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£ÙˆÙ„ÙŠ: {len(df):,}")


duplicates = df.duplicated(subset=["Serial Number", "Date Recorded", "Address"], keep=False).sum()
print(f" Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…ÙƒØ±Ø±Ø©: {duplicates}")

df = df.drop_duplicates(subset=["Serial Number", "Date Recorded", "Address"], keep="first")
print(f" Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {len(df):,}")

# Ù†Ù…Ù„Ø£ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ù€ "Unknown"
text_cols = ["Property Type", "Residential Type", "Non Use Code", "Assessor Remarks", "OPM remarks", "Location"]
for col in text_cols:
    if df[col].dtype.name == "category":
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© "Unknown" Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
        if "Unknown" not in df[col].cat.categories:
            df[col] = df[col].cat.add_categories(["Unknown"])
        #  Ù†Ù…Ù„Ø£ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ©
        df[col] = df[col].fillna("Unknown")
    else:
        # Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
        df[col] = df[col].fillna("Unknown")


#  Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³Ù†Ø© ÙˆØ§Ù„Ø´Ù‡Ø± Ù…Ù† Ø§Ù„ØªØ§Ø±ÙŠØ®
df["Year"] = df["Date Recorded"].dt.year
df["Month"] = df["Date Recorded"].dt.month
print("\nğŸ“† ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­ (Year, Month)")

print("\nÙ…Ù„Ø®Øµ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:")
print(df.info(memory_usage="deep"))
print(df.head())

"""Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§ÙÙŠ

Ø§Ù„Ø¥Ø­ØµØ§Ø¡Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
"""

print("  Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©:")
print(df[["Assessed Value", "Sale Amount", "Sales Ratio"]].describe().T)

print("\n Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¯Ù† Ø§Ù„ÙØ±ÙŠØ¯Ø©:", df["Town"].nunique())
print(" Ø¹Ø¯Ø¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª:", df["Property Type"].nunique())
print(" Ø¹Ø¯Ø¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø³ÙƒÙ†:", df["Residential Type"].nunique())

print("\n Ø£ÙƒØ«Ø± 5 Ù…Ø¯Ù† Ù…Ù† Ø­ÙŠØ« Ø¹Ø¯Ø¯ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¨ÙŠØ¹:")
print(df["Town"].value_counts().head(5))

"""Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø³Ù†Ø©"""

import matplotlib.pyplot as plt

sales_by_year = df.groupby("Year")["Sale Amount"].sum()

plt.figure(figsize=(10,5))
sales_by_year.plot(kind="bar", color="skyblue")
plt.title("Total sales by year")
plt.xlabel("year")
plt.ylabel("Total sales")
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.show()

"""Ø­Ø³Ø¨ Ø§Ù„Ø´Ù‡Ø± Ù„ÙƒÙ„ Ø§Ù„Ø³Ù†ÙˆØ§Øª"""

sales_by_month = df.groupby("Month")["Sale Amount"].sum()

plt.figure(figsize=(10,5))
sales_by_month.plot(kind="bar", color="orange")
plt.title("Total sales by month")
plt.xlabel("month")
plt.ylabel("Total sales")
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.show()

"""Ø­Ø³Ø¨ Ø§Ù„Ù…Ø¯Ù†"""

sales_by_town = df.groupby("Town")["Sale Amount"].sum().sort_values(ascending=False).head(10)

plt.figure(figsize=(12,6))
sales_by_town.plot(kind="bar", color="green")
plt.title(" Top 10 cities in terms of total sales")
plt.xlabel("cities")
plt.ylabel("total sales (USD)")
plt.xticks(rotation=45)
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.show()

"""Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª"""

sales_by_type = df.groupby("Property Type")["Sale Amount"].sum().sort_values(ascending=False)

plt.figure(figsize=(10,5))
sales_by_type.plot(kind="bar", color="purple")
plt.title("Total sales by property type")
plt.xlabel("property type")
plt.ylabel("Total sales (USD)")
plt.xticks(rotation=45)
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.show()

filtered = df[df["Property Type"] != "Unknown"]
sales_by_type = filtered.groupby("Property Type")["Sale Amount"].sum().sort_values(ascending=False)

plt.figure(figsize=(10,5))
sales_by_type.plot(kind="bar", color="purple")
plt.title("Total sales by property type (excluding Unknown)")
plt.xlabel("Property Type")
plt.ylabel("Total Sales (USD)")
plt.xticks(rotation=45)
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.show()

df["Property Type"].value_counts().head(10).plot(kind="bar", color="teal")
plt.title("Number of records by property type")
plt.ylabel("Count of records")
plt.show()

"""Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„ØªÙ‚ÙŠÙŠÙ…ÙŠØ© ÙˆØ³Ø¹Ø± Ø§Ù„Ø¨ÙŠØ¹"""

import seaborn as sns

plt.figure(figsize=(8,6))
sns.scatterplot(x="Assessed Value", y="Sale Amount", data=df.sample(5000), alpha=0.5)
plt.xscale('log')
plt.yscale('log')
plt.title("Relationship between assessed and sale price (log scale)")
plt.xlabel("Assessed Value (log scale)")
plt.ylabel("Sale Amount (log scale)")
plt.show()

filtered = df[(df["Assessed Value"] < 2_000_000) & (df["Sale Amount"] < 2_000_000)]
sns.scatterplot(x="Assessed Value", y="Sale Amount", data=filtered.sample(5000), alpha=0.5)
plt.title("Relationship between assessed and sale price (filtered data)")
plt.show()

correlation = df["Assessed Value"].corr(df["Sale Amount"])
print(f" Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÙŠÙ† Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆØ³Ø¹Ø± Ø§Ù„Ø¨ÙŠØ¹: {correlation:.2f}")

"""ØªÙˆØ²ÙŠØ¹ Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª"""

sns.histplot(df["Sale Amount"], bins=50, color="teal")
plt.yscale('log')
plt.title("Distribution of sales prices")
plt.xlabel("Sale Amount (USD)")
plt.ylabel("Number of transactions(log)")
plt.show()

summary_stats = df[["Assessed Value", "Sale Amount", "Sales Ratio"]].describe()
summary_stats.to_csv("/content/drive/MyDrive/Colab Notebooks/sales_summary_stats.csv")
print("ØªÙ…")

"""Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…

Regression ØªÙ†Ø¨Ø¤ Ø¨Ø³Ø¹Ø± Ø§Ù„Ø¨ÙŠØ¹
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

#  Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙÙŠØ¯Ø© Ø§Ù„ØªÙŠ Ù†Ø­ØªØ§Ø¬
model_df = df[["Assessed Value", "Sale Amount", "Town", "Property Type", "Residential Type"]].copy()

# Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
model_df = model_df.dropna()

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†ØµÙŠØ© Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… (Encoding)
model_df = pd.get_dummies(model_df, drop_first=True)

# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
X = model_df.drop("Sale Amount", axis=1)
y = model_df["Sale Amount"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ØªØ¯Ø±ÙŠØ¨
model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f" RÂ² Score: {r2:.3f}")
print(f" RMSE: {rmse:,.2f} USD")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

data = df.copy()

# Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©
filtered = data[(data["Sale Amount"] < 2_000_000) & (data["Assessed Value"] < 2_000_000)].copy()

filtered["Log_Sale"] = np.log1p(filtered["Sale Amount"])
filtered["Log_Assessed"] = np.log1p(filtered["Assessed Value"])

# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ù‡Ù…Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
features = ["Log_Assessed", "Year", "Sales Ratio"]
filtered = filtered.dropna(subset=features + ["Log_Sale"])

X = filtered[features]
y = filtered["Log_Sale"]

# ØªÙ‚Ø³ÙŠÙ… Train/Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

lin_model = LinearRegression()
lin_model.fit(X_train, y_train)

y_pred_lin = lin_model.predict(X_test)

# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡
r2_lin = r2_score(y_test, y_pred_lin)
rmse_lin = np.sqrt(mean_squared_error(y_test, y_pred_lin))

print(" Linear Regression Results:")
print(f"RÂ² Score: {r2_lin:.3f}")
print(f"RMSE (log-scale): {rmse_lin:.3f}")

# Ù…Ù†  log Ø§Ù„Ù‰ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©
y_test_real = np.expm1(y_test)
y_pred_real = np.expm1(y_pred_lin)

plt.figure(figsize=(6,6))
sns.scatterplot(x=y_test_real, y=y_pred_real, alpha=0.4)
plt.xlabel("Actual Sale Amount (USD)")
plt.ylabel("Predicted Sale Amount (USD)")
plt.title("Linear Regression: Actual vs Predicted")
plt.plot([0, max(y_test_real)], [0, max(y_test_real)], 'r--')
plt.show()

rf_model = RandomForestRegressor(
    n_estimators=150,
    max_depth=None,
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

r2_rf = r2_score(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))

print(f"RÂ² Score: {r2_rf:.3f}")
print(f"RMSE (log-scale): {rmse_rf:.3f}")

importances = rf_model.feature_importances_
feature_names = X.columns

plt.figure(figsize=(6,4))
sns.barplot(x=importances, y=feature_names, color="skyblue")
plt.title("Feature Importance - Random Forest")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.show()

"""Clustering ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø³ÙˆÙ‚"""

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Ù†Ø®ØªØ§Ø± Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø¯ÙŠØ© ÙÙ‚Ø·
cluster_df = df[["Assessed Value", "Sale Amount", "Sales Ratio"]].copy()
cluster_df = cluster_df.dropna()

# Ù…Ù‚ÙŠØ§Ø³ Ù…ÙˆØ­Ø¯
scaler = StandardScaler()
X_scaled = scaler.fit_transform(cluster_df)

# ØªØ¬Ø±Ø¨Ø© K-Means Ù…Ø¹ 4 Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
df["Cluster"] = kmeans.fit_predict(X_scaled)

df["Cluster"].value_counts()

sns.scatterplot(
    x="Assessed Value",
    y="Sale Amount",
    hue="Cluster",
    data=df.sample(5000),
    palette="viridis"
)
plt.title("KMeans Clusters: Property Groups")
plt.show()

"""ØªØ¬Ù‡ÙŠØ² Ù…Ù„Ù Power BI"""

import pandas as pd
import numpy as np

#   Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
export_df = filtered.copy()

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
export_df["Predicted_Linear"] = np.expm1(lin_model.predict(X))
export_df["Predicted_RF"] = np.expm1(rf_model.predict(X))

# Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ø³Ø¨Ø© Ø¨ÙŠÙ† Ø³Ø¹Ø± Ø§Ù„Ø¨ÙŠØ¹ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…
export_df["Sale_to_Assess_Ratio"] = export_df["Sale Amount"] / export_df["Assessed Value"]

# Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤ (Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØºØ§Ø¨Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©)
export_df["Error_RF"] = export_df["Sale Amount"] - export_df["Predicted_RF"]

#  Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
final_columns = [
    "Serial Number", "Date Recorded", "Year", "Month", "Town", "Address",
    "Assessed Value", "Sale Amount", "Sales Ratio", "Property Type", "Residential Type",
    "Predicted_Linear", "Predicted_RF", "Sale_to_Assess_Ratio", "Error_RF"
]

final_df = export_df[final_columns]

# Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
output_path = "/content/drive/MyDrive/Colab Notebooks/RealEstate_Final_For_PowerBI.csv"
final_df.to_csv(output_path, index=False)

print(f" ØªÙ… Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ù†Ø¬Ø§Ø­ Ø¥Ù„Ù‰: {output_path}")
print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù„Ù: {len(final_df):,}")

"""Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ Google Sheets"""

df.to_csv("/content/RealEstate.csv", index=False)

from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default
from gspread_dataframe import set_with_dataframe
import pandas as pd

file_path = "/content/drive/MyDrive/Colab Notebooks/RealEstate_Final_For_PowerBI.csv"
df = pd.read_csv(file_path)

# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙ‚Ø·
cols_to_keep = [
    "Town",
    "Property Type",
    "Assessed Value",
    "Sale Amount",
    "Year",
    "Month"
]
df_small = df[cols_to_keep].head(800000)

creds, _ = default()

# Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Google Sheets
gc = gspread.authorize(creds)

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯ ÙÙŠ Google Drive
sh = gc.create("RealEstate_Sales_Data")

worksheet = sh.get_worksheet(0)

set_with_dataframe(worksheet, df_small)

print("âœ… ØªÙ… Ø±ÙØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ Ø¥Ù„Ù‰ Google Sheets!")
print("ğŸ“„ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù„Ù:", sh.url)
